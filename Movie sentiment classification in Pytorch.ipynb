{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data loading and preparation\n",
    "\n",
    "As we mentioned the very first step is to prepare the data in a way that: (1) is understood by our predictive model (neural net in our case), and (2) let us evaluate our progress.\n",
    "\n",
    "For the purpose of the workshop, we have created an easy way to download, process, and split the data for training.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we need to import torchtext, which is the utility we used for\n",
    "# working with the IMDB dataset for sentiment classification\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structure\n",
    "\n",
    "Our raw data, contains examples of the form: (1) input text sequence, and (2) label, which can be 0 for negative reviews or 1 for positive reviews.\n",
    "\n",
    "Let's reflect these two fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our first field contains raw text, we tokenize this text using\n",
    "# spaCy, a widely-used Python library for NLP\n",
    "SEQUENCE_LENGTH = 200\n",
    "TEXT = data.Field(tokenize='spacy',fix_length=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our second field contains the expected labels for each piece of text\n",
    "LABEL = data.Field(sequential=False,\n",
    "                  use_vocab=False,\n",
    "                  preprocessing=(lambda s: int(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "Now, using these two fields we can read the dataset and attach the data points to our fields, and the .splits() will create the splits for us: in this case we will work only with test and training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset splits\n"
     ]
    }
   ],
   "source": [
    "train, dev = datasets.IMDB.splits(TEXT, LABEL, root='.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Story',\n",
       " 'of',\n",
       " 'a',\n",
       " 'man',\n",
       " 'who',\n",
       " 'has',\n",
       " 'unnatural',\n",
       " 'feelings',\n",
       " 'for',\n",
       " 'a',\n",
       " 'pig',\n",
       " '.',\n",
       " 'Starts',\n",
       " 'out',\n",
       " 'with',\n",
       " 'a',\n",
       " 'opening',\n",
       " 'scene',\n",
       " 'that',\n",
       " 'is',\n",
       " 'a',\n",
       " 'terrific',\n",
       " 'example',\n",
       " 'of',\n",
       " 'absurd',\n",
       " 'comedy',\n",
       " '.',\n",
       " 'A',\n",
       " 'formal',\n",
       " 'orchestra',\n",
       " 'audience',\n",
       " 'is',\n",
       " 'turned',\n",
       " 'into',\n",
       " 'an',\n",
       " 'insane',\n",
       " ',',\n",
       " 'violent',\n",
       " 'mob',\n",
       " 'by',\n",
       " 'the',\n",
       " 'crazy',\n",
       " 'chantings',\n",
       " 'of',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'singers',\n",
       " '.',\n",
       " 'Unfortunately',\n",
       " 'it',\n",
       " 'stays',\n",
       " 'absurd',\n",
       " 'the',\n",
       " 'WHOLE',\n",
       " 'time',\n",
       " 'with',\n",
       " 'no',\n",
       " 'general',\n",
       " 'narrative',\n",
       " 'eventually',\n",
       " 'making',\n",
       " 'it',\n",
       " 'just',\n",
       " 'too',\n",
       " 'off',\n",
       " 'putting',\n",
       " '.',\n",
       " 'Even',\n",
       " 'those',\n",
       " 'from',\n",
       " 'the',\n",
       " 'era',\n",
       " 'should',\n",
       " 'be',\n",
       " 'turned',\n",
       " 'off',\n",
       " '.',\n",
       " 'The',\n",
       " 'cryptic',\n",
       " 'dialogue',\n",
       " 'would',\n",
       " 'make',\n",
       " 'Shakespeare',\n",
       " 'seem',\n",
       " 'easy',\n",
       " 'to',\n",
       " 'a',\n",
       " 'third',\n",
       " 'grader',\n",
       " '.',\n",
       " 'On',\n",
       " 'a',\n",
       " 'technical',\n",
       " 'level',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'better',\n",
       " 'than',\n",
       " 'you',\n",
       " 'might',\n",
       " 'think',\n",
       " 'with',\n",
       " 'some',\n",
       " 'good',\n",
       " 'cinematography',\n",
       " 'by',\n",
       " 'future',\n",
       " 'great',\n",
       " 'Vilmos',\n",
       " 'Zsigmond',\n",
       " '.',\n",
       " 'Future',\n",
       " 'stars',\n",
       " 'Sally',\n",
       " 'Kirkland',\n",
       " 'and',\n",
       " 'Frederic',\n",
       " 'Forrest',\n",
       " 'can',\n",
       " 'be',\n",
       " 'seen',\n",
       " 'briefly',\n",
       " '.']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's see some examples in our train split\n",
    "train[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Once',\n",
       " 'again',\n",
       " 'Mr.',\n",
       " 'Costner',\n",
       " 'has',\n",
       " 'dragged',\n",
       " 'out',\n",
       " 'a',\n",
       " 'movie',\n",
       " 'for',\n",
       " 'far',\n",
       " 'longer',\n",
       " 'than',\n",
       " 'necessary',\n",
       " '.',\n",
       " 'Aside',\n",
       " 'from',\n",
       " 'the',\n",
       " 'terrific',\n",
       " 'sea',\n",
       " 'rescue',\n",
       " 'sequences',\n",
       " ',',\n",
       " 'of',\n",
       " 'which',\n",
       " 'there',\n",
       " 'are',\n",
       " 'very',\n",
       " 'few',\n",
       " 'I',\n",
       " 'just',\n",
       " 'did',\n",
       " 'not',\n",
       " 'care',\n",
       " 'about',\n",
       " 'any',\n",
       " 'of',\n",
       " 'the',\n",
       " 'characters',\n",
       " '.',\n",
       " 'Most',\n",
       " 'of',\n",
       " 'us',\n",
       " 'have',\n",
       " 'ghosts',\n",
       " 'in',\n",
       " 'the',\n",
       " 'closet',\n",
       " ',',\n",
       " 'and',\n",
       " 'Costner',\n",
       " \"'s\",\n",
       " 'character',\n",
       " 'are',\n",
       " 'realized',\n",
       " 'early',\n",
       " 'on',\n",
       " ',',\n",
       " 'and',\n",
       " 'then',\n",
       " 'forgotten',\n",
       " 'until',\n",
       " 'much',\n",
       " 'later',\n",
       " ',',\n",
       " 'by',\n",
       " 'which',\n",
       " 'time',\n",
       " 'I',\n",
       " 'did',\n",
       " 'not',\n",
       " 'care',\n",
       " '.',\n",
       " 'The',\n",
       " 'character',\n",
       " 'we',\n",
       " 'should',\n",
       " 'really',\n",
       " 'care',\n",
       " 'about',\n",
       " 'is',\n",
       " 'a',\n",
       " 'very',\n",
       " 'cocky',\n",
       " ',',\n",
       " 'overconfident',\n",
       " 'Ashton',\n",
       " 'Kutcher',\n",
       " '.',\n",
       " 'The',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'he',\n",
       " 'comes',\n",
       " 'off',\n",
       " 'as',\n",
       " 'kid',\n",
       " 'who',\n",
       " 'thinks',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'better',\n",
       " 'than',\n",
       " 'anyone',\n",
       " 'else',\n",
       " 'around',\n",
       " 'him',\n",
       " 'and',\n",
       " 'shows',\n",
       " 'no',\n",
       " 'signs',\n",
       " 'of',\n",
       " 'a',\n",
       " 'cluttered',\n",
       " 'closet',\n",
       " '.',\n",
       " 'His',\n",
       " 'only',\n",
       " 'obstacle',\n",
       " 'appears',\n",
       " 'to',\n",
       " 'be',\n",
       " 'winning',\n",
       " 'over',\n",
       " 'Costner',\n",
       " '.',\n",
       " 'Finally',\n",
       " 'when',\n",
       " 'we',\n",
       " 'are',\n",
       " 'well',\n",
       " 'past',\n",
       " 'the',\n",
       " 'half',\n",
       " 'way',\n",
       " 'point',\n",
       " 'of',\n",
       " 'this',\n",
       " 'stinker',\n",
       " ',',\n",
       " 'Costner',\n",
       " 'tells',\n",
       " 'us',\n",
       " 'all',\n",
       " 'about',\n",
       " 'Kutcher',\n",
       " \"'s\",\n",
       " 'ghosts',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'told',\n",
       " 'why',\n",
       " 'Kutcher',\n",
       " 'is',\n",
       " 'driven',\n",
       " 'to',\n",
       " 'be',\n",
       " 'the',\n",
       " 'best',\n",
       " 'with',\n",
       " 'no',\n",
       " 'prior',\n",
       " 'inkling',\n",
       " 'or',\n",
       " 'foreshadowing',\n",
       " '.',\n",
       " 'No',\n",
       " 'magic',\n",
       " 'here',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " 'all',\n",
       " 'I',\n",
       " 'could',\n",
       " 'do',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'from',\n",
       " 'turning',\n",
       " 'it',\n",
       " 'off',\n",
       " 'an',\n",
       " 'hour',\n",
       " 'in',\n",
       " '.']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train[0].label # First example has negative sentiment\n",
    "dev[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data: build a vocabulary\n",
    "\n",
    "When we work with text, we usually transform our words into ids and keep a vocabulary which associates words with ids. Using torchtext this is an easy task. We also want to start with a relatively small vocabulary. A classical way to do this is to include only words above a certain frequency. Let's do this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MIN_FREQ = 10\n",
    "TEXT.build_vocab(train, min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data: create mini-batches\n",
    "\n",
    "Modern neural networks are usually trained using minibatches. These minibatches are used for optimizing the weights using backpropagation.\n",
    "\n",
    "Torchtext also facilitates creating an iterator over these minibatches. Typical sizes for NLP are 16,32, 64, depending on the size of the data and the task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_iter, dev_iter = data.BucketIterator.splits((train, dev),\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  sort_key=lambda x: len(x.text),\n",
    "                                                  device=-1,\n",
    "                                                  repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model definition\n",
    "\n",
    "Now we have our data ready, let's start defining our neural network structure. \n",
    "\n",
    "The core neural network components in Pytorch are in the nn module. This modules include typical layers: Linear, RNNs, CNNs, etc. which can be combined to create a multilayer neural network.\n",
    "\n",
    "Every model we create extends the basid nn.Module, and implements at least two methods:\n",
    "\n",
    "- **init**: Defines the core variables of our network.\n",
    "- **forward**: Defines the \"forward\" pass. This is, the computation made by our network to transform the input data into a prediction output.\n",
    "\n",
    "Let's create our first model, a simple sentiment classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can start defining our predictive model. The first step is to define the 'architecture' of the model\n",
    "# and its main operations with the data that goes through the network.\n",
    "\n",
    "# The core neural network components of Pytorch belong to the nn module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Let's start with a very simple baseline model\n",
    "class BaseSentimentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_size, output_dim, batch_size=32, debug=None):\n",
    "        super(BaseSentimentClassifier, self).__init__()\n",
    "        self.embed = nn.Embedding(input_dim, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_dim)\n",
    "        \n",
    "        self.debug = debug\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # The forward pass defines how the input data is processed by the network\n",
    "        # to make a prediction\n",
    "        if (self.debug):\n",
    "            print(input)\n",
    "        embed = self.embed(input)\n",
    "        if (self.debug):\n",
    "            print(embed)\n",
    "        # This operation summarizes a 3D tensor 200x32x200 into a 32x200 matrix\n",
    "        out = F.max_pool1d(embed.transpose(0,2), input.size()[0]).squeeze().transpose(0,1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training process\n",
    "\n",
    "Now that we have defined the architecture of our network, we can start defining our training process. Ideally, this training process should be independent of our model architecture. A very naive approach would be to define a function which receives a model instance. Let's do this:. But first, let's define an auxiliary method for showing progress (do not worry much about this method now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log(time, epoch, iterations, batch_idx, train_iter, loss, train_acc, dev_loss=None, dev_acc=None):\n",
    "    header = '  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy'\n",
    "    dev_log_template = ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{:8.6f},{:12.4f},{:12.4f}'.split(','))\n",
    "    log_template =     ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{},{:12.4f},{}'.split(','))\n",
    "    print(header)\n",
    "    if(dev_loss):\n",
    "        print(dev_log_template.format(time,\n",
    "                    epoch, iterations, 1+batch_idx, len(train_iter),\n",
    "                    100. * (1+batch_idx) / len(train_iter), loss.data[0], dev_loss, train_acc, dev_acc))\n",
    "    else:\n",
    "        print(log_template.format(time,\n",
    "                    epoch, iterations, 1+batch_idx, len(train_iter),\n",
    "                    100. * (1+batch_idx) / len(train_iter), loss.data[0], ' '*8, train_acc, ' '*12))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, batches, num_epochs=2, learning_rate = 0.001):\n",
    "    import time\n",
    "    train_iter, dev_iter = batches\n",
    "    \n",
    "    # First we need to define our loss/objective function\n",
    "    # Cross Entropy Loss already applies softmax\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # And the optimizer (Gradient-descent methods)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    from torch.autograd import Variable\n",
    "    # Now the code for training our network\n",
    "    iterations = 0\n",
    "    log_every = 300\n",
    "    dev_every = 700\n",
    "    start = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_iter.init_epoch()\n",
    "        n_correct, n_total = 0, 0\n",
    "        for batch_idx, batch in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch.text)\n",
    "            iterations += 1\n",
    "            n_correct += (torch.max(output, 1)[1].view(batch.label.size()).data == batch.label.data).sum()\n",
    "            n_total += batch.batch_size\n",
    "            train_acc = 100. * n_correct/n_total\n",
    "            loss = criterion(output, batch.label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if iterations % log_every == 0:\n",
    "                log(time.time()-start, \n",
    "                    epoch, \n",
    "                    iterations, \n",
    "                    batch_idx, \n",
    "                    train_iter, \n",
    "                    loss, \n",
    "                    train_acc)\n",
    "            if iterations % dev_every == 0:\n",
    "                model.eval(); dev_iter.init_epoch()\n",
    "                n_dev_correct, dev_loss = 0, 0\n",
    "                for dev_batch_idx, dev_batch in enumerate(dev_iter):\n",
    "                    answer = model(dev_batch.text)\n",
    "                    n_dev_correct += (torch.max(answer, 1)[1].view(dev_batch.label.size()).data == dev_batch.label.data).sum()\n",
    "                    dev_loss = criterion(answer, dev_batch.label)\n",
    "                dev_acc = 100. * n_dev_correct / len(dev)\n",
    "                log(time.time()-start, \n",
    "                        epoch, \n",
    "                        iterations, \n",
    "                        batch_idx, \n",
    "                        train_iter, \n",
    "                        loss, \n",
    "                        train_acc,\n",
    "                        dev_loss.data[0],\n",
    "                        dev_acc)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Finally training\n",
    "\n",
    "We can finally use our method for training and try out different models. Let's start with our simple sentiment classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy\n",
      "    12     0       300   300/782        38% 0.681308               52.3646             \n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy\n",
      "    23     0       600   600/782        77% 0.686829               53.5156             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input dimensions are defined by the len of the input vocab\n",
    "input_dim = len(TEXT.vocab)\n",
    "hidden_size = 200\n",
    "# Output dimensions are two: 0 = negative, 1 = positive\n",
    "output_dim = 2\n",
    "\n",
    "model = BaseSentimentClassifier(input_dim,\n",
    "                              hidden_size,\n",
    "                              output_dim)\n",
    "\n",
    "# Let's call the training process\n",
    "train(model, (train_iter, dev_iter),num_epochs=4, learning_rate = 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Saving your model and vocabulary for serving predictions\n",
    "\n",
    "Imagine we are satisfied with our last model and we want to serve predictions in production. All we need to do is to: (1) save our model, and (2) save our vocabulary for turning words into ids understood by our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save model\n",
    "torch.save(model, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save vocab: strings to ids dictionary\n",
    "import dill as pickle\n",
    "input_vocab = TEXT.vocab.stoi\n",
    "\n",
    "vocab = input_vocab\n",
    "with open('input_vocab.pickle',  'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Predict!\n",
    "\n",
    "Now imagine you were happy with the model and want to serve predictions on production. Typically, this will happen on another environment/service than the one you trained your model on. But you are lucky you stored all the data you need. Let's do this!\n",
    "\n",
    "First, load the vocab and the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "with open('input_vocab.pickle', 'rb') as file_:\n",
    "    vocab = pickle.load(file_)\n",
    "\n",
    "production_model = torch.load('model.pt')\n",
    "production_model.debug=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to predict the sentiment of unseen text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "\n",
    "zeros = [0 for i in range(0,200)]\n",
    "def text2ids(texts):\n",
    "    def padding(text):\n",
    "        t = text.split(\" \")\n",
    "        wids = [vocab[s.lower()] for s in t]\n",
    "        padded = wids + zeros[len(wids):]\n",
    "        return padded\n",
    "    \n",
    "    batch = [padding(text) for text in texts]\n",
    "    tensor = torch.LongTensor(batch).transpose(0,1)\n",
    "    \n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "text = text2ids([\"The movie was very terribly bad\", \"The movie was very terribly bad\"])\n",
    "\n",
    "prediction = production_model(text)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Exercise: Define a RNN model\n",
    "\n",
    "Recurrent neural networks are thought for dealing with sequences, and thus are suitable for NLP tasks: sentences are sequences of words, texts are sequences of sentences, etc.\n",
    "\n",
    "Try creating an new model using some of the advanced RNN provided by Pytorch: LSTM (with or without bidirection) or the simpler but effective GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's start with a very simple baseline model\n",
    "class RNNSentimentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_size, output_dim, batch_size=32):\n",
    "        super(NewSentimentClassifier, self).__init__()\n",
    "        self.embed = nn.Embedding(input_dim, hidden_size)\n",
    "        # Here you should define an RNN layer: be careful with embed shape and hidden, output shapes\n",
    "        # self.rnn\n",
    "        self.fc2 = nn.Linear(hidden_size, output_dim)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # The forward pass defines how the input data is processed by the network\n",
    "        # to make a prediction\n",
    "        embed = self.embed(input)\n",
    "        # Here you should pass the batches throught the RNN layer: you might need to use .view\n",
    "        # out = self.rnn ...\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
